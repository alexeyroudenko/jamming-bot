{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-12-15T09:37:41.126326Z",
     "end_time": "2023-12-15T09:38:39.298973Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# use array visited\n",
    "#\n",
    "site_url = \"https://arthew0.ru/\"\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import tldextract\n",
    "\n",
    "urls = [site_url]\n",
    "visited = []\n",
    "def clean_url(url):\n",
    "    return url.replace(\"www.\", \"\")\n",
    "\n",
    "while len(urls) != 0:\n",
    "    random.shuffle(urls)\n",
    "    current_url = urls.pop()\n",
    "    visited.append(current_url)\n",
    "    current_domain = urlparse(current_url).hostname\n",
    "    current_site = urlparse(current_url).scheme + \"://\" + urlparse(current_url).netloc\n",
    "    try:\n",
    "        response = requests.get(current_url, timeout=1)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\", from_encoding=\"utf-8\")\n",
    "        print(len(urls), len(visited), current_domain, current_url)\n",
    "        link_elements = soup.select(\"a[href]\")\n",
    "        for link_element in link_elements:\n",
    "            url = link_element['href']\n",
    "            url = requests.compat.urljoin(current_site, url)\n",
    "            url = clean_url(url)\n",
    "            if not url in visited and not \"javascript\" in url:\n",
    "                urls.append(url)\n",
    "        if len(visited) > 1000:\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "# database check\n",
    "#\n",
    "create = False\n",
    "fill = False\n",
    "db_name = \"d1b.db\"\n",
    "\n",
    "def get_values(url):\n",
    "    return {\"hostname\": urlparse(url).hostname, \"url\": url, \"visited\":0}\n",
    "\n",
    "if create:\n",
    "    import os\n",
    "    os.remove(db_name)\n",
    "\n",
    "from databases import Database\n",
    "database = Database(f'sqlite+aiosqlite:///{db_name}')\n",
    "\n",
    "if create:\n",
    "    await database.connect()\n",
    "    query = \"\"\"CREATE TABLE Urls (id INTEGER PRIMARY KEY, hostname VARCHAR(127), url VARCHAR(127), visited INTEGER)\"\"\"\n",
    "    await database.execute(query=query)\n",
    "    query = \"\"\"CREATE TABLE Domains (id INTEGER PRIMARY KEY AUTOINCREMENT, hostname VARCHAR(127), visited INTEGER)\"\"\"\n",
    "    await database.execute(query=query)\n",
    "\n",
    "if fill:\n",
    "    for i in range(0, 3):\n",
    "        values = get_values(f\"https://arthew0.ru/page{i}\")\n",
    "        values['visited'] = i % 2\n",
    "        query = \"INSERT INTO Urls(hostname, url, visited) VALUES (:hostname, :url, :visited)\"\n",
    "        await database.execute(query=query, values=values)\n",
    "    for i in range(0, 15):\n",
    "        values = get_values(f\"https://ya.ru/page{i}\")\n",
    "        values['visited'] = i % 2\n",
    "        query = \"INSERT INTO Urls(hostname, url, visited) VALUES (:hostname, :url, :visited)\"\n",
    "        await database.execute(query=query, values=values)\n",
    "\n",
    "# next url:\n",
    "query = \"SELECT id, hostname, url, count(visited) FROM Urls where visited==0 GROUP BY hostname ORDER BY count(visited) LIMIT 1\"\n",
    "rows = await database.fetch_all(query=query)\n",
    "url_id = rows[0][0]\n",
    "next_url = rows[0][2]\n",
    "print('next_url:', url_id, next_url)\n",
    "\n",
    "query = f\"UPDATE Urls SET visited=1 WHERE id={url_id}\"\n",
    "await database.execute(query=query)\n",
    "\n",
    "#query = \"SELECT * FROM Urls\"\n",
    "#rows = await database.fetch_all(query=query)\n",
    "#print('urls:', rows)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-15T10:46:52.491239Z",
     "end_time": "2023-12-15T10:46:52.521237Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "db_name = \"db.db\"\n",
    "from databases import Database\n",
    "database = Database(f'sqlite+aiosqlite:///{db_name}')\n",
    "await database.connect()\n",
    "query = \"SELECT * FROM Urls LIMIT 1000,2000\"\n",
    "rows = await database.fetch_all(query=query)\n",
    "# print('urls:', rows)\n",
    "\n",
    "display(rows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-15T11:02:07.720039Z",
     "end_time": "2023-12-15T11:02:07.745039Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# work version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_url(url):\n",
    "    return url.replace(\"www.\", \"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-16T07:36:16.163415Z",
     "end_time": "2023-12-16T07:36:16.176415Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "filters = []\n",
    "with open('top500Domains.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='\\\"')\n",
    "    for row in spamreader:\n",
    "        filters.append(clean_url(''.join(row[1])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-16T07:36:17.276750Z",
     "end_time": "2023-12-16T07:36:17.290749Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import tldextract\n",
    "from datetime import date\n",
    "from tld import get_tld\n",
    "\n",
    "#\n",
    "# use database visited\n",
    "#\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "create = True\n",
    "fill = False\n",
    "db_name = f\"db_{date_time}.db\"\n",
    "\n",
    "def get_values(url):\n",
    "    return {\"hostname\": urlparse(url).hostname, \"url\": url, \"visited\":0}\n",
    "\n",
    "if create:\n",
    "    import os\n",
    "    if os.path.exists(db_name):\n",
    "        os.remove(db_name)\n",
    "\n",
    "from databases import Database\n",
    "database = Database(f'sqlite+aiosqlite:///{db_name}')\n",
    "await database.connect()\n",
    "\n",
    "if create:\n",
    "    query = \"\"\"CREATE TABLE Urls (id INTEGER PRIMARY KEY, hostname VARCHAR(127), url VARCHAR(127) unique, src_url VARCHAR(127), visited INTEGER)\"\"\"\n",
    "    await database.execute(query=query)\n",
    "\n",
    "values = get_values(f\"https://arthew0.ru/\")\n",
    "query = \"INSERT INTO Urls(hostname, url, visited) VALUES (:hostname, :url, :visited)\"\n",
    "await database.execute(query=query, values=values)\n",
    "\n",
    "\n",
    "#site_url = \"https://arthew0.ru/\"\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import tldextract\n",
    "\n",
    "step = 0\n",
    "while True:\n",
    "    step = step + 1\n",
    "    query = \"SELECT id, hostname, url, src_url, count(visited) FROM Urls where visited==0 GROUP BY hostname ORDER BY count(visited) LIMIT 1\"\n",
    "    rows = await database.fetch_all(query=query)\n",
    "    try:\n",
    "        url_id = rows[0][0]\n",
    "        current_url = rows[0][2]\n",
    "        src_url = rows[0][3]\n",
    "        query = f\"UPDATE Urls SET visited=1 WHERE id={url_id}\"\n",
    "        await database.execute(query=query)\n",
    "        current_domain = urlparse(current_url).hostname\n",
    "        current_site = urlparse(current_url).scheme + \"://\" + urlparse(current_url).netloc\n",
    "        res = get_tld(current_url, as_object=True)\n",
    "        current_base_domain = res.fld\n",
    "        try:\n",
    "            response = requests.get(current_url, timeout=1)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\", from_encoding=\"utf-8\")\n",
    "            print(step, src_url, \">\" ,current_url)\n",
    "            link_elements = soup.select(\"a[href]\")\n",
    "            for link_element in link_elements:\n",
    "                url = link_element['href']\n",
    "                url = requests.compat.urljoin(current_site, url)\n",
    "                url = clean_url(url)\n",
    "                if not \"javascript\" in url and not current_base_domain in filters:\n",
    "                    values = get_values(url)\n",
    "                    values['src_url'] = current_url\n",
    "                    query = \"INSERT OR IGNORE INTO Urls(hostname, url, src_url, visited) VALUES (:hostname, :url, :src_url, :visited)\"\n",
    "                    await database.execute(query=query, values=values)\n",
    "                if step > 50:\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Exception1:\", e)\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception2:\", e)\n",
    "        pass\n",
    "\n",
    "database.disconnect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-16T07:48:55.923727Z",
     "end_time": "2023-12-16T07:48:56.516250Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "database.disconnect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-16T07:33:32.579703Z",
     "end_time": "2023-12-16T07:33:32.593702Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-16T08:49:29.087414Z",
     "end_time": "2023-12-16T08:49:29.087414Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
