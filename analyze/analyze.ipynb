{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# start = 10000\n",
    "# for i in range(start, start+100000):\n",
    "#     idd = i\n",
    "#     url = f\"http://localhost:8003/api/v1/tags/{idd}/\"\n",
    "#     r = requests.delete(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_tags(): \n",
    "    url = \"http://localhost:8003/api/v1/tags/\"\n",
    "    r = requests.get(url)\n",
    "    print(r.json())\n",
    "get_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests    \n",
    "def delete_tags(): \n",
    "    url = \"http://localhost:8003/api/v1/tags/\"\n",
    "    r = requests.get(url)\n",
    "    for t in r.json():\n",
    "        idd = t['id']\n",
    "        urld = f\"http://localhost:8003/api/v1/tags/{idd}/\"\n",
    "        r = requests.delete(urld)     \n",
    "delete_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add tags from steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import glob\n",
    "files = glob.glob(\"../data/path/steps/*\")\n",
    "files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "\n",
    "for file in files[0:1000]:        \n",
    "    contents = open(file).readlines()\n",
    "    step = int(contents[0].strip())    \n",
    "    print(file, step)   \n",
    "    text_path = files_txt[step]     \n",
    "    text = open(files_txt[step-1]).read().strip().replace(\"\\n\",\"\")\n",
    "    \n",
    "    url_semantic = f\"http://localhost:8005/api/v1/semantic/tags/\"\n",
    "    headers = {'content-type': 'application/json'}\n",
    "    rr = requests.post(url_semantic, data = json.dumps({\"text\": text}), headers=headers)                \n",
    "    data = rr.json()\n",
    "    # if \"words\" in data.keys(): \n",
    "    #     words = rr.json()['words']\n",
    "    # if \"hrases\" in data.keys(): \n",
    "    #     hrases = rr.json()['hrases']\n",
    "    if \"sim\" in data.keys(): \n",
    "        sim = rr.json()['sim']\n",
    "    # print(words, hrases, sim)\n",
    "    \n",
    "\n",
    "    for hras in sim:       \n",
    "        url = \"http://localhost:8003/api/v1/tags/\"\n",
    "        headers = {'content-type': 'application/json'}\n",
    "        data = {'name': hras, \"count\": 0}\n",
    "        response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "    \n",
    "    if step > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def add_tag(tag):\n",
    "    url = \"http://localhost:8003/api/v1/tags/\"\n",
    "    headers = {'content-type': 'application/json'}\n",
    "    data = {'name': tag, \"count\": 0}\n",
    "    response = requests.post(url, data=json.dumps(data), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_gext(html):\n",
    "    html = html.encode('utf-8')\n",
    "    soup = BeautifulSoup(html, \"html.parser\", from_encoding=\"utf-8\")           \n",
    "    # headings = [h.get_text() for h in soup.find_all(['h1', 'h2', 'h3'])]\n",
    "    # head = \" \".join(headings)\n",
    "    paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "    # text_out = head + \" \".join(paragraphs)\n",
    "    text_out = \" \".join(paragraphs[0:4])    \n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import requests\n",
    "# import glob\n",
    "# files = glob.glob(\"../data/path/steps/*\")\n",
    "# files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "# files_html = glob.glob(\"../data/path/html/*\")\n",
    "\n",
    "# for file in files[0:1000]:        \n",
    "#     contents = open(file).readlines()\n",
    "#     step = int(contents[0].strip())    \n",
    "#     print(file, step)\n",
    "#     # text_prev = open(files_txt[step-1]).read().strip().replace(\"\\n\",\"\")\n",
    "#     html_path = f'../data/path/html/{str(step).zfill(8)}.html'        \n",
    "#     html = open(html_path).read()\n",
    "#     text_new = get_gext(html).strip().replace(\"\\n\",\"\")    \n",
    "#     text_path = f'../data/path/txt/{str(step).zfill(8)}.txt'     \n",
    "#     open(text_path, \"w\").writelines(text_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import requests\n",
    "# url = \"http://localhost:8003/api/v1/tags/tags/group/\"\n",
    "# response = requests.get(url).json()\n",
    "# out = \"\"\n",
    "# for t in response:\n",
    "#     out += str(t['name']).lower() + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41 added token.text a: 0.39\n",
      "step: 41 added token.text so: 0.59\n",
      "step: 41 added token.text you: 0.57\n",
      "step: 41 added token.text can: 0.49\n",
      "step: 41 added token.text the: 0.44\n",
      "step: 41 added token.text best: 0.47\n",
      "step: 41 added token.text possible: 0.41\n",
      "step: 41 added token.text experiences: 0.54\n",
      "step: 41 added token.text .: 0.37\n",
      "step: 44 added token.text Influence: 0.41\n",
      "step: 44 added token.text 's: 0.37\n",
      "step: 44 added token.text future: 0.55\n",
      "step: 44 added token.text .: 0.37\n",
      "step: 44 added token.text some: 0.47\n",
      "step: 44 added token.text of: 0.47\n",
      "step: 44 added token.text our: 0.53\n",
      "step: 44 added token.text in: 0.40\n",
      "step: 44 added token.text and: 0.50\n",
      "step: 44 added token.text learning: 0.47\n",
      "step: 44 added token.text .: 0.37\n",
      "step: 44 added token.text the: 0.44\n",
      "step: 44 added token.text and: 0.50\n",
      "step: 44 added token.text for: 0.42\n",
      "step: 44 added token.text of: 0.47\n",
      "step: 44 added token.text .: 0.37\n",
      "step: 44 added token.text Own: 0.56\n",
      "step: 44 added token.text and: 0.50\n",
      "step: 44 added token.text and: 0.50\n",
      "step: 44 added token.text .: 0.37\n",
      "step: 45 added token.text To: 0.45\n",
      "step: 45 added token.text that: 0.57\n",
      "step: 45 added token.text help: 0.45\n",
      "step: 45 added token.text of: 0.47\n",
      "step: 45 added token.text people: 0.55\n",
      "step: 45 added token.text reach: 0.38\n",
      "step: 45 added token.text their: 0.49\n",
      "step: 45 added token.text goals: 0.35\n",
      "step: 45 added token.text we: 0.56\n",
      "step: 45 added token.text bringing: 0.51\n",
      "step: 45 added token.text together: 0.52\n",
      "step: 45 added token.text people: 0.55\n",
      "step: 45 added token.text everywhere: 0.51\n",
      "step: 45 added token.text and: 0.50\n",
      "step: 45 added token.text empowering: 0.37\n",
      "step: 45 added token.text them: 0.52\n",
      "step: 45 added token.text to: 0.45\n",
      "step: 45 added token.text do: 0.48\n",
      "step: 45 added token.text their: 0.49\n",
      "step: 45 added token.text best: 0.47\n",
      "step: 45 added token.text work: 0.51\n",
      "step: 45 added token.text .: 0.37\n",
      "step: 45 added token.text ”: 0.37\n",
      "step: 45 added token.text Engagement: 0.35\n",
      "step: 45 added token.text To: 0.45\n",
      "step: 45 added token.text that: 0.57\n",
      "step: 45 added token.text help: 0.45\n",
      "step: 45 added token.text of: 0.47\n",
      "step: 45 added token.text people: 0.55\n",
      "step: 45 added token.text reach: 0.38\n",
      "step: 45 added token.text their: 0.49\n",
      "step: 45 added token.text goals: 0.35\n",
      "step: 45 added token.text we: 0.56\n",
      "step: 45 added token.text bringing: 0.51\n",
      "step: 45 added token.text together: 0.52\n",
      "step: 45 added token.text people: 0.55\n",
      "step: 45 added token.text everywhere: 0.51\n",
      "step: 45 added token.text and: 0.50\n",
      "step: 45 added token.text empowering: 0.37\n",
      "step: 45 added token.text them: 0.52\n",
      "step: 45 added token.text to: 0.45\n",
      "step: 45 added token.text do: 0.48\n",
      "step: 45 added token.text their: 0.49\n",
      "step: 45 added token.text best: 0.47\n",
      "step: 45 added token.text work: 0.51\n",
      "step: 45 added token.text .: 0.37\n",
      "step: 45 added token.text ”: 0.37\n",
      "step: 46 added token.text We: 0.56\n",
      "step: 46 added token.text with: 0.46\n",
      "step: 46 added token.text that: 0.57\n",
      "step: 46 added token.text are: 0.41\n",
      "step: 46 added token.text often: 0.46\n",
      "step: 46 added token.text so: 0.59\n",
      "step: 46 added token.text that: 0.57\n",
      "step: 46 added token.text what: 0.60\n",
      "step: 46 added token.text we: 0.56\n",
      "step: 46 added token.text works: 0.39\n",
      "step: 46 added token.text for: 0.42\n",
      "step: 46 added token.text everyone: 0.64\n",
      "step: 46 added token.text .: 0.37\n",
      "step: 46 added token.text First: 0.39\n",
      "step: 46 added token.text we: 0.56\n",
      "step: 46 added token.text working: 0.41\n",
      "step: 46 added token.text hard: 0.47\n",
      "step: 46 added token.text to: 0.45\n",
      "step: 46 added token.text our: 0.53\n",
      "step: 46 added token.text can: 0.49\n",
      "step: 46 added token.text and: 0.50\n",
      "step: 46 added token.text for: 0.42\n",
      "step: 46 added token.text all: 0.56\n",
      "step: 46 added token.text of: 0.47\n",
      "step: 46 added token.text our: 0.53\n",
      "step: 46 added token.text throughout: 0.36\n",
      "step: 46 added token.text the: 0.44\n",
      "step: 46 added token.text .: 0.37\n",
      "step: 46 added token.text we: 0.56\n",
      "step: 46 added token.text deep: 0.44\n",
      "step: 46 added token.text where: 0.49\n",
      "step: 46 added token.text we: 0.56\n",
      "step: 46 added token.text see: 0.48\n",
      "step: 46 added token.text and: 0.50\n",
      "step: 46 added token.text our: 0.53\n",
      "step: 46 added token.text .: 0.37\n",
      "step: 46 added token.text And: 0.50\n",
      "step: 46 added token.text finally: 0.47\n",
      "step: 46 added token.text we: 0.56\n",
      "step: 46 added token.text 're: 0.51\n",
      "step: 46 added token.text sharing: 0.50\n",
      "step: 46 added token.text what: 0.60\n",
      "step: 46 added token.text we: 0.56\n",
      "step: 46 added token.text learn: 0.49\n",
      "step: 46 added token.text so: 0.59\n",
      "step: 46 added token.text that: 0.57\n",
      "step: 46 added token.text it: 0.56\n",
      "step: 46 added token.text easier: 0.39\n",
      "step: 46 added token.text for: 0.42\n",
      "step: 46 added token.text anyone: 0.49\n",
      "step: 46 added token.text to: 0.45\n",
      "step: 46 added token.text truly: 0.62\n",
      "step: 46 added token.text be: 0.47\n",
      "step: 46 added token.text to: 0.45\n",
      "step: 46 added token.text everyone: 0.64\n",
      "step: 46 added token.text .: 0.37\n",
      "step: 46 added token.text year: 0.38\n",
      "step: 46 added token.text old: 0.40\n",
      "step: 46 added token.text a: 0.39\n",
      "step: 46 added token.text with: 0.46\n",
      "step: 46 added token.text the: 0.44\n",
      "step: 46 added token.text of: 0.47\n",
      "step: 46 added token.text Blind: 0.36\n",
      "step: 46 added token.text People: 0.55\n",
      "step: 46 added token.text and: 0.50\n",
      "step: 46 added token.text the: 0.44\n",
      "step: 46 added token.text of: 0.47\n",
      "step: 46 added token.text the: 0.44\n",
      "step: 46 added token.text Blind: 0.36\n",
      "step: 46 added token.text .: 0.37\n",
      "step: 46 added token.text To: 0.45\n",
      "step: 46 added token.text that: 0.57\n",
      "step: 46 added token.text benefits: 0.39\n",
      "step: 46 added token.text everyone: 0.64\n",
      "step: 46 added token.text the: 0.44\n",
      "step: 46 added token.text and: 0.50\n",
      "step: 46 added token.text our: 0.53\n",
      "step: 46 added token.text need: 0.51\n",
      "step: 46 added token.text to: 0.45\n",
      "step: 46 added token.text reflect: 0.43\n",
      "step: 46 added token.text those: 0.54\n",
      "step: 46 added token.text who: 0.49\n",
      "step: 46 added token.text it: 0.56\n",
      "step: 46 added token.text and: 0.50\n",
      "step: 46 added token.text feel: 0.64\n",
      "step: 46 added token.text to: 0.45\n",
      "step: 46 added token.text do: 0.48\n",
      "step: 46 added token.text their: 0.49\n",
      "step: 46 added token.text best: 0.47\n",
      "step: 46 added token.text work: 0.51\n",
      "step: 46 added token.text .: 0.37\n",
      "step: 46 added token.text That: 0.57\n",
      "step: 46 added token.text 's: 0.37\n",
      "step: 46 added token.text why: 0.54\n",
      "step: 46 added token.text we: 0.56\n",
      "step: 46 added token.text 're: 0.51\n",
      "step: 46 added token.text to: 0.45\n",
      "step: 46 added token.text our: 0.53\n",
      "step: 46 added token.text so: 0.59\n",
      "step: 46 added token.text that: 0.57\n",
      "step: 46 added token.text everyone: 0.64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m added token.text \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m                 sim\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m---> 32\u001b[0m                 \u001b[43madd_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# html_path = f'../data/path/html/{str(step).zfill(8)}.html'        \u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# html = open(html_path).read()\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# text_new = get_gext(html).strip().replace(\"\\n\",\"\")    \u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# text_path = f'../data/path/txt/{str(step).zfill(8)}.txt'     \u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# open(text_path, \"w\").writelines(text_new)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m, in \u001b[0;36madd_tag\u001b[1;34m(tag)\u001b[0m\n\u001b[0;32m      6\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: tag, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Enviroments\\mlflow\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import glob\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "files = glob.glob(\"../data/path/steps/*\")\n",
    "files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "files_html = glob.glob(\"../data/path/html/*\")\n",
    "\n",
    "# Happiness: joy, bliss, delight, euphoria, serenity, contentment.\n",
    "# Love: affection, passion, devotion, adoration, tenderness, romance.\n",
    "# Life: existence, being, journey, reality, vitality, soul.\n",
    "\n",
    "for file in files[0:512]:        \n",
    "    contents = open(file).readlines()\n",
    "    step = int(contents[0].strip())        \n",
    "    text = open(files_txt[step-1]).read().strip().replace(\"\\n\",\"\")\n",
    "    if text != \"\":\n",
    "        # print(file, step, text[0:128])                \n",
    "        doc = nlp(text)                                                        \n",
    "        query_text = \"happiness love life\"                \n",
    "        query = nlp(query_text)\n",
    "        sim = [] \n",
    "        for token in doc:\n",
    "            if token.has_vector and query.has_vector:\n",
    "                similarity = query.similarity(token)\n",
    "                if similarity > 0.35:\n",
    "                    print(f\"step: {step} added token.text {token.text}: {similarity:.2f}\")\n",
    "                    sim.append(token.text)\n",
    "                    add_tag(token.text)\n",
    "\n",
    "    \n",
    "    # html_path = f'../data/path/html/{str(step).zfill(8)}.html'        \n",
    "    # html = open(html_path).read()\n",
    "    # text_new = get_gext(html).strip().replace(\"\\n\",\"\")    \n",
    "    # text_path = f'../data/path/txt/{str(step).zfill(8)}.txt'     \n",
    "    # open(text_path, \"w\").writelines(text_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import requests\n",
    "# import glob\n",
    "# files = glob.glob(\"../data/path/steps/*\")\n",
    "# files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "# files_html = glob.glob(\"../data/path/html/*\")\n",
    "# ip = \"\"\n",
    "\n",
    "# do_geo = False\n",
    "# do_words = False\n",
    "# do_tags = False\n",
    "\n",
    "# do_geo = True\n",
    "# do_words = True\n",
    "# do_tags = True\n",
    "\n",
    "# # delete_tags()\n",
    "\n",
    "# import spacy\n",
    "# nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "# nlp_qu = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# with open(f'../data/path/collect.tsv', 'w') as out_file:\n",
    "                    \n",
    "#     for file in files:\n",
    "#         contents = open(file).readlines()\n",
    "#         step = int(contents[0].strip())\n",
    "                \n",
    "#         # Analyze text\n",
    "#         text = open(files_txt[step-1]).read().strip().replace(\"\\n\",\"\")        \n",
    "#         html = open(files_html[step-1]).read()\n",
    "#         text_new = get_gext(html).strip().replace(\"\\n\",\"\") \n",
    "#         text = text_new\n",
    "        \n",
    "#         if text != \"\":                                    \n",
    "#             doc = nlp_lg(text)                                                        \n",
    "#             query_text = \"happiness love life joy bliss delight euphoria serenity contentment.\"                \n",
    "#             query = nlp_qu(query_text)\n",
    "#             sim = []  \n",
    "#             # similarities = {}\n",
    "#             for token in doc:\n",
    "#                 if token.has_vector and query.has_vector:\n",
    "#                     similarity = query.similarity(token)\n",
    "#                     # if similarity > 0:\n",
    "#                     #     similarities[token.text] = similarity\n",
    "#                     if similarity > 0.5:\n",
    "#                         print(f\"step: {step} added token.text {token.text}: {similarity:.2f}\")\n",
    "#                         # sim.append(token.text)\n",
    "#                         # add_tag(token.text)\n",
    "                    \n",
    "#         # if do_tags:              \n",
    "#         #     for hras in sim:       \n",
    "#         #         url = \"http://localhost:8003/api/v1/tags/\"\n",
    "#         #         headers = {'content-type': 'application/json'}\n",
    "#         #         data = {'name': hras, \"count\": 0}\n",
    "#         #         response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "\n",
    "        \n",
    "#         # if step > 100:\n",
    "#         #     break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import requests\n",
    "# import glob\n",
    "# files = glob.glob(\"../data/path/steps/*\")\n",
    "# files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "# files_html = glob.glob(\"../data/path/html/*\")\n",
    "# ip = \"\"\n",
    "\n",
    "# do_geo = False\n",
    "# do_words = False\n",
    "# do_tags = False\n",
    "\n",
    "# do_geo = True\n",
    "# do_words = True\n",
    "# do_tags = True\n",
    "\n",
    "# # delete_tags()\n",
    "\n",
    "# import spacy\n",
    "# nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "# nlp_qu = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# with open(f'../data/path/collect.tsv', 'w') as out_file:\n",
    "                    \n",
    "#     for file in files:\n",
    "#         contents = open(file).readlines()\n",
    "#         step = int(contents[0].strip())\n",
    "        \n",
    "        \n",
    "#         # Analyze text\n",
    "#         text = open(files_txt[step-1]).read().strip().replace(\"\\n\",\"\")        \n",
    "#         html = open(files_html[step-1]).read()\n",
    "#         text_new = get_gext(html).strip().replace(\"\\n\",\"\") \n",
    "#         text = text_new\n",
    "                                    \n",
    "#         doc = nlp_lg(text)\n",
    "        \n",
    "#         # Find named entities, phrases and concepts\n",
    "#         # for entity in doc.ents:\n",
    "#             # print(entity.text, entity.label_)\n",
    "#         # print(text)\n",
    "#         # print(text_new)\n",
    "#         # print(\"------------\")\n",
    "\n",
    "\n",
    "        \n",
    "#         # code = contents[3].strip()\n",
    "#         # if code == \"200\":\n",
    "#         #     ip = contents[4].strip()        \n",
    "#         # latitude = \"0\"\n",
    "#         # longitude = \"0\"\n",
    "#         # city = \"\"\n",
    "        \n",
    "#         # if do_geo:                \n",
    "#         #     url = f\"http://localhost:8004/api/v1/ip/{ip}/\"\n",
    "#         #     r = requests.get(url)\n",
    "#         #     latitude = str(r.json()['latitude'])\n",
    "#         #     longitude = str(r.json()['longitude'])\n",
    "#         #     city = r.json()['city']\n",
    "            \n",
    "            \n",
    "        \n",
    "#         # words = []\n",
    "#         # hrases = []\n",
    "#         # sim = []\n",
    "                    \n",
    "#         # if text != \"\":            \n",
    "#         #     if do_words:\n",
    "#         #         url_semantic = f\"http://localhost:8005/api/v1/semantic/tags/\"\n",
    "#         #         headers = {'content-type': 'application/json'}\n",
    "#         #         rr = requests.post(url_semantic, data = json.dumps({\"text\": text}), headers=headers)                \n",
    "#         #         data = rr.json()\n",
    "#         #         if \"words\" in data.keys(): \n",
    "#         #             words = rr.json()['words']\n",
    "#         #         if \"hrases\" in data.keys(): \n",
    "#         #             hrases = rr.json()['hrases']\n",
    "#                 # if \"sim\" in data.keys(): \n",
    "#                 #     sim = rr.json()['sim']\n",
    "                             \n",
    "#         sim = []      \n",
    "#         query_text = \"happiness love life joy bliss delight euphoria serenity contentment.\"\n",
    "#         query = nlp_qu(query_text)\n",
    "#         similarities = {}\n",
    "#         for token in doc:\n",
    "#             if token.has_vector and query.has_vector:\n",
    "#                 similarity = query.similarity(token)\n",
    "#                 if similarity > 0:\n",
    "#                     similarities[token.text] = similarity\n",
    "#                 if similarity > 0.6:\n",
    "#                     print(f\"step: {step} added token.text {token.text}: {similarity:.2f}\")\n",
    "\n",
    "\n",
    "#         # sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "#         # print(f\"Similarity w '{query.text}':\")\n",
    "#         # for word, similarity in sorted_similarities[0:5]:\n",
    "#         #     print(f\"{word}: {similarity:.2f}\")\n",
    "#         #     sim.append(word)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             # if do_tags:              \n",
    "#             #     for hras in sim:       \n",
    "#             #         url = \"http://localhost:8003/api/v1/tags/\"\n",
    "#             #         headers = {'content-type': 'application/json'}\n",
    "#             #         data = {'name': hras, \"count\": 0}\n",
    "#             #         response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "\n",
    "#         # out = [contents[0].strip(), contents[1].strip(), contents[2].strip(), contents[3].strip(), contents[4].strip(), latitude, longitude, city, json.dumps(words), json.dumps(hrases), json.dumps(sim)] #text\n",
    "#         # out_file.write('\\t'.join(out)+\"\\n\")\n",
    "#         # print(text)\n",
    "#         # print(out)        \n",
    "#         # print(json.dumps(sim))\n",
    "        \n",
    "#         if step > 100:\n",
    "#             break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
