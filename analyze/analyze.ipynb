{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# start = 10000\n",
    "# for i in range(start, start+100000):\n",
    "#     idd = i\n",
    "#     url = f\"http://localhost:8003/api/v1/tags/{idd}/\"\n",
    "#     r = requests.delete(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests    \n",
    "def delete_tags(): \n",
    "    url = \"http://localhost:8003/api/v1/tags/\"\n",
    "    r = requests.get(url)\n",
    "    for t in r.json():\n",
    "        idd = t['id']\n",
    "        urld = f\"http://localhost:8003/api/v1/tags/{idd}/\"\n",
    "        r = requests.delete(urld)     \n",
    "delete_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def add_tag(tag):\n",
    "    url = \"http://localhost:8003/api/v1/tags/\"\n",
    "    headers = {'content-type': 'application/json'}\n",
    "    data = {'name': tag, \"count\": 0}\n",
    "    response = requests.post(url, data=json.dumps(data), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_gext(html):\n",
    "    html = html.encode('utf-8')\n",
    "    soup = BeautifulSoup(html, \"html.parser\", from_encoding=\"utf-8\")           \n",
    "    # headings = [h.get_text() for h in soup.find_all(['h1', 'h2', 'h3'])]\n",
    "    # head = \" \".join(headings)\n",
    "    paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "    # text_out = head + \" \".join(paragraphs)\n",
    "    text_out = \" \".join(paragraphs[0:4])    \n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import requests\n",
    "# import glob\n",
    "# files = glob.glob(\"../data/path/steps/*\")\n",
    "# files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "# files_html = glob.glob(\"../data/path/html/*\")\n",
    "\n",
    "# for file in files[0:1000]:        \n",
    "#     contents = open(file).readlines()\n",
    "#     step = int(contents[0].strip())    \n",
    "#     print(file, step)\n",
    "#     # text_prev = open(files_txt[step-1]).read().strip().replace(\"\\n\",\"\")\n",
    "#     html_path = f'../data/path/html/{str(step).zfill(8)}.html'        \n",
    "#     html = open(html_path).read()\n",
    "#     text_new = get_gext(html).strip().replace(\"\\n\",\"\")    \n",
    "#     text_path = f'../data/path/txt/{str(step).zfill(8)}.txt'     \n",
    "#     open(text_path, \"w\").writelines(text_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import requests\n",
    "# url = \"http://localhost:8003/api/v1/tags/tags/group/\"\n",
    "# response = requests.get(url).json()\n",
    "# out = \"\"\n",
    "# for t in response:\n",
    "#     out += str(t['name']).lower() + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import glob\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "files = glob.glob(\"../data/path/steps/*\")\n",
    "files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "files_html = glob.glob(\"../data/path/html/*\")\n",
    "\n",
    "# Happiness: joy, bliss, delight, euphoria, serenity, contentment.\n",
    "# Love: affection, passion, devotion, adoration, tenderness, romance.\n",
    "# Life: existence, being, journey, reality, vitality, soul.\n",
    "\n",
    "for file in files[0:512]:        \n",
    "    contents = open(file).readlines()\n",
    "    step = int(contents[0].strip())        \n",
    "    text = open(files_txt[step-1]).read().strip().replace(\"\\n\",\"\")\n",
    "    if text != \"\":\n",
    "        # print(file, step, text[0:128])                \n",
    "        doc = nlp(text)                                                        \n",
    "        query_text = \"happiness love life\"                \n",
    "        query = nlp(query_text)\n",
    "        sim = [] \n",
    "        for token in doc:\n",
    "            if token.has_vector and query.has_vector:\n",
    "                similarity = query.similarity(token)\n",
    "                if similarity > 0.35:\n",
    "                    print(f\"step: {step} added token.text {token.text}: {similarity:.2f}\")\n",
    "                    sim.append(token.text)\n",
    "                    add_tag(token.text)\n",
    "\n",
    "    \n",
    "    # html_path = f'../data/path/html/{str(step).zfill(8)}.html'        \n",
    "    # html = open(html_path).read()\n",
    "    # text_new = get_gext(html).strip().replace(\"\\n\",\"\")    \n",
    "    # text_path = f'../data/path/txt/{str(step).zfill(8)}.txt'     \n",
    "    # open(text_path, \"w\").writelines(text_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import requests\n",
    "# import glob\n",
    "# files = glob.glob(\"../data/path/steps/*\")\n",
    "# files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "# files_html = glob.glob(\"../data/path/html/*\")\n",
    "# ip = \"\"\n",
    "\n",
    "# do_geo = False\n",
    "# do_words = False\n",
    "# do_tags = False\n",
    "\n",
    "# do_geo = True\n",
    "# do_words = True\n",
    "# do_tags = True\n",
    "\n",
    "# # delete_tags()\n",
    "\n",
    "# import spacy\n",
    "# nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "# nlp_qu = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# with open(f'../data/path/collect.tsv', 'w') as out_file:\n",
    "                    \n",
    "#     for file in files:\n",
    "#         contents = open(file).readlines()\n",
    "#         step = int(contents[0].strip())\n",
    "                \n",
    "#         # Analyze text\n",
    "#         text = open(files_txt[step-1]).read().strip().replace(\"\\n\",\"\")        \n",
    "#         html = open(files_html[step-1]).read()\n",
    "#         text_new = get_gext(html).strip().replace(\"\\n\",\"\") \n",
    "#         text = text_new\n",
    "        \n",
    "#         if text != \"\":                                    \n",
    "#             doc = nlp_lg(text)                                                        \n",
    "#             query_text = \"happiness love life joy bliss delight euphoria serenity contentment.\"                \n",
    "#             query = nlp_qu(query_text)\n",
    "#             sim = []  \n",
    "#             # similarities = {}\n",
    "#             for token in doc:\n",
    "#                 if token.has_vector and query.has_vector:\n",
    "#                     similarity = query.similarity(token)\n",
    "#                     # if similarity > 0:\n",
    "#                     #     similarities[token.text] = similarity\n",
    "#                     if similarity > 0.5:\n",
    "#                         print(f\"step: {step} added token.text {token.text}: {similarity:.2f}\")\n",
    "#                         # sim.append(token.text)\n",
    "#                         # add_tag(token.text)\n",
    "                    \n",
    "#         # if do_tags:              \n",
    "#         #     for hras in sim:       \n",
    "#         #         url = \"http://localhost:8003/api/v1/tags/\"\n",
    "#         #         headers = {'content-type': 'application/json'}\n",
    "#         #         data = {'name': hras, \"count\": 0}\n",
    "#         #         response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "\n",
    "        \n",
    "#         # if step > 100:\n",
    "#         #     break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import requests\n",
    "# import glob\n",
    "# files = glob.glob(\"../data/path/steps/*\")\n",
    "# files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "# files_html = glob.glob(\"../data/path/html/*\")\n",
    "# ip = \"\"\n",
    "\n",
    "# do_geo = False\n",
    "# do_words = False\n",
    "# do_tags = False\n",
    "\n",
    "# do_geo = True\n",
    "# do_words = True\n",
    "# do_tags = True\n",
    "\n",
    "# # delete_tags()\n",
    "\n",
    "# import spacy\n",
    "# nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "# nlp_qu = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# with open(f'../data/path/collect.tsv', 'w') as out_file:\n",
    "                    \n",
    "#     for file in files:\n",
    "#         contents = open(file).readlines()\n",
    "#         step = int(contents[0].strip())\n",
    "        \n",
    "        \n",
    "#         # Analyze text\n",
    "#         text = open(files_txt[step-1]).read().strip().replace(\"\\n\",\"\")        \n",
    "#         html = open(files_html[step-1]).read()\n",
    "#         text_new = get_gext(html).strip().replace(\"\\n\",\"\") \n",
    "#         text = text_new\n",
    "                                    \n",
    "#         doc = nlp_lg(text)\n",
    "        \n",
    "#         # Find named entities, phrases and concepts\n",
    "#         # for entity in doc.ents:\n",
    "#             # print(entity.text, entity.label_)\n",
    "#         # print(text)\n",
    "#         # print(text_new)\n",
    "#         # print(\"------------\")\n",
    "\n",
    "\n",
    "        \n",
    "#         # code = contents[3].strip()\n",
    "#         # if code == \"200\":\n",
    "#         #     ip = contents[4].strip()        \n",
    "#         # latitude = \"0\"\n",
    "#         # longitude = \"0\"\n",
    "#         # city = \"\"\n",
    "        \n",
    "#         # if do_geo:                \n",
    "#         #     url = f\"http://localhost:8004/api/v1/ip/{ip}/\"\n",
    "#         #     r = requests.get(url)\n",
    "#         #     latitude = str(r.json()['latitude'])\n",
    "#         #     longitude = str(r.json()['longitude'])\n",
    "#         #     city = r.json()['city']\n",
    "            \n",
    "            \n",
    "        \n",
    "#         # words = []\n",
    "#         # hrases = []\n",
    "#         # sim = []\n",
    "                    \n",
    "#         # if text != \"\":            \n",
    "#         #     if do_words:\n",
    "#         #         url_semantic = f\"http://localhost:8005/api/v1/semantic/tags/\"\n",
    "#         #         headers = {'content-type': 'application/json'}\n",
    "#         #         rr = requests.post(url_semantic, data = json.dumps({\"text\": text}), headers=headers)                \n",
    "#         #         data = rr.json()\n",
    "#         #         if \"words\" in data.keys(): \n",
    "#         #             words = rr.json()['words']\n",
    "#         #         if \"hrases\" in data.keys(): \n",
    "#         #             hrases = rr.json()['hrases']\n",
    "#                 # if \"sim\" in data.keys(): \n",
    "#                 #     sim = rr.json()['sim']\n",
    "                                    \n",
    "\n",
    "            \n",
    "            \n",
    "#         sim = []      \n",
    "#         query_text = \"happiness love life joy bliss delight euphoria serenity contentment.\"\n",
    "#         query = nlp_qu(query_text)\n",
    "#         similarities = {}\n",
    "#         for token in doc:\n",
    "#             if token.has_vector and query.has_vector:\n",
    "#                 similarity = query.similarity(token)\n",
    "#                 if similarity > 0:\n",
    "#                     similarities[token.text] = similarity\n",
    "#                 if similarity > 0.6:\n",
    "#                     print(f\"step: {step} added token.text {token.text}: {similarity:.2f}\")\n",
    "\n",
    "\n",
    "#         # sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "#         # print(f\"Similarity w '{query.text}':\")\n",
    "#         # for word, similarity in sorted_similarities[0:5]:\n",
    "#         #     print(f\"{word}: {similarity:.2f}\")\n",
    "#         #     sim.append(word)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             # if do_tags:              \n",
    "#             #     for hras in sim:       \n",
    "#             #         url = \"http://localhost:8003/api/v1/tags/\"\n",
    "#             #         headers = {'content-type': 'application/json'}\n",
    "#             #         data = {'name': hras, \"count\": 0}\n",
    "#             #         response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "\n",
    "#         # out = [contents[0].strip(), contents[1].strip(), contents[2].strip(), contents[3].strip(), contents[4].strip(), latitude, longitude, city, json.dumps(words), json.dumps(hrases), json.dumps(sim)] #text\n",
    "#         # out_file.write('\\t'.join(out)+\"\\n\")\n",
    "#         # print(text)\n",
    "#         # print(out)        \n",
    "#         # print(json.dumps(sim))\n",
    "        \n",
    "#         if step > 100:\n",
    "#             break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
