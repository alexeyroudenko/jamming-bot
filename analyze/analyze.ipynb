{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "start = 10000\n",
    "for i in range(start, start+100000):\n",
    "    idd = i\n",
    "    url = f\"http://localhost:8003/api/v1/tags/{idd}/\"\n",
    "    r = requests.delete(url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests    \n",
    "def delete_tags(): \n",
    "    url = \"http://localhost:8003/api/v1/tags/\"\n",
    "    r = requests.get(url)\n",
    "    for t in r.json():\n",
    "        idd = t['id']\n",
    "        urld = f\"http://localhost:8003/api/v1/tags/{idd}/\"\n",
    "        r = requests.delete(urld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import glob\n",
    "files = glob.glob(\"../data/path/steps/*\")\n",
    "files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "files_html = glob.glob(\"../data/path/html/*\")\n",
    "ip = \"\"\n",
    "\n",
    "do_geo = False\n",
    "do_words = False\n",
    "do_tags = False\n",
    "\n",
    "do_geo = True\n",
    "do_words = True\n",
    "do_tags = True\n",
    "\n",
    "delete_tags()\n",
    "\n",
    "def get_gext(html):\n",
    "    html = html.encode('utf-8')\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\", from_encoding=\"utf-8\")           \n",
    "    # headings = [h.get_text() for h in soup.find_all(['h1', 'h2', 'h3'])]\n",
    "    # head = \" \".join(headings)\n",
    "    paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "    # text_out = head + \" \".join(paragraphs)\n",
    "    text_out = \" \".join(paragraphs)    \n",
    "    return text_out\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "with open(f'../data/path/collect.tsv', 'w') as out_file:\n",
    "                    \n",
    "    for file in files:\n",
    "        contents = open(file).readlines()\n",
    "        step = int(contents[0].strip())\n",
    "        \n",
    "        \n",
    "        # Analyze text\n",
    "        text = open(files_txt[step-1]).read().strip().replace(\"\\n\",\"\")        \n",
    "        html = open(files_html[step-1]).read()\n",
    "        text_new = get_gext(html).strip().replace(\"\\n\",\"\") \n",
    "        text = text_new\n",
    "        \n",
    "        \n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Find named entities, phrases and concepts\n",
    "        # for entity in doc.ents:\n",
    "            # print(entity.text, entity.label_)\n",
    "        # print(text)\n",
    "        # print(text_new)\n",
    "        # print(\"------------\")\n",
    "        \n",
    "        code = contents[3].strip()\n",
    "        if code == \"200\":\n",
    "            ip = contents[4].strip()        \n",
    "        latitude = \"0\"\n",
    "        longitude = \"0\"\n",
    "        city = \"\"\n",
    "        \n",
    "        if do_geo:                \n",
    "            url = f\"http://localhost:8004/api/v1/ip/{ip}/\"\n",
    "            r = requests.get(url)\n",
    "            latitude = str(r.json()['latitude'])\n",
    "            longitude = str(r.json()['longitude'])\n",
    "            city = r.json()['city']\n",
    "        \n",
    "        words = []\n",
    "        hrases = []\n",
    "        sim = []\n",
    "                    \n",
    "        if text != \"\":            \n",
    "            if do_words:\n",
    "                url_semantic = f\"http://localhost:8005/api/v1/semantic/tags/\"\n",
    "                headers = {'content-type': 'application/json'}\n",
    "                rr = requests.post(url_semantic, data = json.dumps({\"text\": text}), headers=headers)                \n",
    "                data = rr.json()\n",
    "                if \"words\" in data.keys(): \n",
    "                    words = rr.json()['words']\n",
    "                if \"hrases\" in data.keys(): \n",
    "                    hrases = rr.json()['hrases']\n",
    "                if \"sim\" in data.keys(): \n",
    "                    sim = rr.json()['sim']\n",
    "                                    \n",
    "\n",
    "            if do_tags:              \n",
    "                for hras in sim:       \n",
    "                    url = \"http://localhost:8003/api/v1/tags/\"\n",
    "                    headers = {'content-type': 'application/json'}\n",
    "                    data = {'name': hras, \"count\": 0}\n",
    "                    response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "\n",
    "        out = [contents[0].strip(), contents[1].strip(), contents[2].strip(), contents[3].strip(), contents[4].strip(), latitude, longitude, city, json.dumps(words), json.dumps(hrases), json.dumps(sim)] #text\n",
    "        out_file.write('\\t'.join(out)+\"\\n\")\n",
    "        # print(text)\n",
    "        print(out)        \n",
    "        \n",
    "        # if step > 100:\n",
    "            # break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
