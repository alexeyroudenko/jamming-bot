{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(corpus):\n",
    "    # Remove punctuations from the corpus\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    corpus = corpus.translate(translator)\n",
    "\n",
    "    # Remove digits from the corpus\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    corpus = corpus.translate(remove_digits)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=10):\n",
    "    from ipywidgets import IntProgress\n",
    "    from IPython.display import display\n",
    "\n",
    "    progress = IntProgress(min=0, max=len(sequence), value=0)\n",
    "    display(progress)\n",
    "    \n",
    "    for index, record in enumerate(sequence):\n",
    "        if index % every == 0:\n",
    "            progress.value = index\n",
    "        yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Установите punkt, если еще не установлено\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def is_sentence_nltk(text):\n",
    "    # Токенизируем текст на предложения\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Проверяем, является ли исходная строка единственным предложением\n",
    "    if len(sentences) != 1 or text.strip() != sentences[0]:\n",
    "        return False\n",
    "    # Проверяем количество слов\n",
    "    words = word_tokenize(text)\n",
    "    # Фильтруем только слова (убираем знаки препинания)\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    if len(words) < 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def get_first_two_words(text):\n",
    "    # Превращаем несколько пробелов в один и убираем лишние пробелы\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Токенизируем строку на слова\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Фильтруем только слова (убираем знаки препинания)\n",
    "    filtered_words = [word for word in words if word.isalnum()]\n",
    "    \n",
    "    # Проверяем, есть ли хотя бы 2 слова\n",
    "    if len(filtered_words) < 2:\n",
    "        return \"Недостаточно слов\"\n",
    "    \n",
    "    # Возвращаем первые 2 слова\n",
    "    return filtered_words[:2]\n",
    "\n",
    "# Примеры\n",
    "# print(is_sentence_nltk(\"I would like to receive the version of Newsletter\"))  # True\n",
    "# print(is_sentence_nltk(\"！！\"))\n",
    "# print(is_sentence_nltk(\"Это предложение. А это другое.\"))  # False\n",
    "# print(is_sentence_nltk(\"Mr. Smith lives here.\"))  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import glob\n",
    "import string\n",
    "files = glob.glob(\"../data/path/steps/*\")\n",
    "files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "\n",
    "import re  \n",
    "def remove_all_cjk(text):\n",
    "    return re.sub(r'[\\u3000-\\u30FF\\u4E00-\\u9FFF\\uAC00-\\uD7AF]', '', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[:;!?“”]', '', text)\n",
    "\n",
    "with open('../data/path/dataset.csv', 'w') as out_file:\n",
    "    out_file.write(\"id,cluster,title,text\\n\")                \n",
    "    for file in files:\n",
    "        contents = open(file).readlines()\n",
    "        step = int(contents[0].strip())\n",
    "        text_path = f'../data/path/txt/{str(step).zfill(8)}.txt'                           \n",
    "        try:\n",
    "            text = open(text_path).read()\n",
    "            text = remove_punctuation(remove_all_cjk(text))\n",
    "            text = text.strip().replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\" \").strip()\n",
    "            text = re.sub(r'\\s+', ' ', text.strip())\n",
    "            text = clean_text(text)\n",
    "            if text!=\"\" and text!=\"For instantly purchase Please contact us to get this domain\":\n",
    "                if is_sentence_nltk(text):\n",
    "                    title = \" \".join(get_first_two_words(text))\n",
    "                    out = [str(step), \"0\", title, \"\\\"\" + str(text)+ \"\\\"\"] \n",
    "                    out_file.write(','.join(out)+\"\\n\")\n",
    "                    \n",
    "                    if step > 50000:\n",
    "                        break\n",
    "        except FileNotFoundError:\n",
    "            ...\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/path/dataset.csv\", on_bad_lines='skip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"title\"].tolist()\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df['text'].tolist()\n",
    "corpus = \" \".join(documents).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_documents = []\n",
    "for document in documents:\n",
    "    document = clean_text(document).lower()\n",
    "    cleaned_documents.append(document)\n",
    "documents = cleaned_documents\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y scikit-learn\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# !pip uninstall -y scipy\n",
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import time\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentCluster:\n",
    "    \"\"\"Class to cluster document\"\"\"\n",
    "    def __init__(self,model,data,labels):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.model_name = self._get_model_name()\n",
    "        \n",
    "        self.cluster_pipeline = None\n",
    "        \n",
    "    \n",
    "    def _get_model_name(self):\n",
    "        \"\"\"Get name of model being used\"\"\"\n",
    "        model_name = str(self.model).split('(')[0]\n",
    "        return model_name\n",
    "            \n",
    "\n",
    "    \n",
    "    def train_model(self,ngram,use_idf):\n",
    "        \"\"\" \n",
    "            Train model\n",
    "            ngram: int\n",
    "            use_idf: bool\n",
    "            analyzer: string\n",
    "        \n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        self._ngram = ngram\n",
    "        self._use_idf = use_idf\n",
    "        \n",
    "        # Set a pipeline\n",
    "        # 1. The first step of the pipeline is to find count vectorizer\n",
    "        # Countvectorizer converts a collection of text documents to a matrix of token counts\n",
    "        # This implementation produces a sparse representation of the counts\n",
    "        # 2. We then use TfidfTransformr\n",
    "        # TfidfTransformer transforms a count matrix to a normalized tf or tf-idf representation\n",
    "        # Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency\n",
    "\n",
    "        self.cluster_pipeline = Pipeline([('vect', CountVectorizer(ngram_range=(1,ngram), \n",
    "                analyzer=\"word\")), ('tfidf', TfidfTransformer(use_idf=use_idf)), \n",
    "                ('model', self.model)])\n",
    "        self._cluster = self.cluster_pipeline.fit(self.data)\n",
    "        \n",
    "        self._train_time = time.time() - current_time\n",
    "        \n",
    "        \n",
    "        print(\"Training completed\")\n",
    "        print(\"Training time for {} : {} secs\".format(self.model_name, self._train_time))\n",
    "        \n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict cluster id for all clusters\n",
    "        \"\"\"\n",
    "        self.label_id_dict = {}\n",
    "        self.document_id_dict = {}\n",
    "        self.cluster_id = []\n",
    "        for i, document in enumerate(self.data):\n",
    "            cluster_id = self.cluster_pipeline.predict([document])[0]\n",
    "            self.label_id_dict.update({self.labels[i]:cluster_id})\n",
    "            self.document_id_dict.update({document:cluster_id})\n",
    "            self.cluster_id.append(cluster_id)\n",
    "        \n",
    "        return self.label_id_dict, self.document_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=cluster_number, init = \"k-means++\", max_iter=150, n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_cluster = DocumentCluster(data=documents,model=model,labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_cluster.train_model(ngram=2, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_label, result_docs = doc_cluster.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_path = '../data/path/dataset.csv'\n",
    "df = pd.read_csv(dataset_path, on_bad_lines='skip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add cluster data to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_path = '../data/path/dataset.csv'\n",
    "df = pd.read_csv(dataset_path, on_bad_lines='skip')\n",
    "df.head()\n",
    "\n",
    "for item in result_docs:\n",
    "    search_string = item\n",
    "    df.loc[df['text'].str.lower() == search_string, 'cluster'] = result_docs[item]\n",
    "        \n",
    "# Сохраняем обновленный DataFrame\n",
    "df.to_csv(dataset_path, index=False) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
