{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(corpus):\n",
    "    # Remove punctuations from the corpus\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    corpus = corpus.translate(translator)\n",
    "\n",
    "    # Remove digits from the corpus\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    corpus = corpus.translate(remove_digits)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import glob\n",
    "import string\n",
    "files = glob.glob(\"../data/path/steps/*\")\n",
    "files_txt = glob.glob(\"../data/path/txt/*\")\n",
    "import re\n",
    "def remove_all_cjk(text):\n",
    "    return re.sub(r'[\\u3000-\\u30FF\\u4E00-\\u9FFF\\uAC00-\\uD7AF]', '', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[:;!?“”]', '', text)\n",
    "\n",
    "with open(f'../data/path/dataset.csv', 'w') as out_file:\n",
    "    out_file.write(\"id,article,text\\n\")                \n",
    "    for file in files:\n",
    "        contents = open(file).readlines()\n",
    "        step = int(contents[0].strip())\n",
    "        \n",
    "        text = open(files_txt[step-1]).read()\n",
    "        text = remove_punctuation(remove_all_cjk(text))\n",
    "        text = text.strip().replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\" \").strip()\n",
    "        text = clean_text(text)\n",
    "        \n",
    "\n",
    "        if text!=\"\":\n",
    "            out = [str(step), text[0:32], \"\\\"\" + str(text)+ \"\\\"\"] \n",
    "            out_file.write(','.join(out)+\"\\n\")\n",
    "            if step > 10000:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/path/dataset.csv\", on_bad_lines='skip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"article\"].tolist()\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df['text'].tolist()\n",
    "corpus = \" \".join(documents).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_documents = []\n",
    "for document in documents:\n",
    "    document = clean_text(document).lower()\n",
    "    cleaned_documents.append(document)\n",
    "documents = cleaned_documents\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import time\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentCluster:\n",
    "    \"\"\"Class to cluster document\"\"\"\n",
    "    def __init__(self,model,data,labels):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.model_name = self._get_model_name()\n",
    "        \n",
    "        self.cluster_pipeline = None\n",
    "        \n",
    "    \n",
    "    def _get_model_name(self):\n",
    "        \"\"\"Get name of model being used\"\"\"\n",
    "        model_name = str(self.model).split('(')[0]\n",
    "        return model_name\n",
    "            \n",
    "\n",
    "    \n",
    "    def train_model(self,ngram,use_idf):\n",
    "        \"\"\" \n",
    "            Train model\n",
    "            ngram: int\n",
    "            use_idf: bool\n",
    "            analyzer: string\n",
    "        \n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        self._ngram = ngram\n",
    "        self._use_idf = use_idf\n",
    "        \n",
    "        # Set a pipeline\n",
    "        # 1. The first step of the pipeline is to find count vectorizer\n",
    "        # Countvectorizer converts a collection of text documents to a matrix of token counts\n",
    "        # This implementation produces a sparse representation of the counts\n",
    "        # 2. We then use TfidfTransformr\n",
    "        # TfidfTransformer transforms a count matrix to a normalized tf or tf-idf representation\n",
    "        # Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency\n",
    "\n",
    "        self.cluster_pipeline = Pipeline([('vect', CountVectorizer(ngram_range=(1,ngram), \n",
    "                analyzer=\"word\")), ('tfidf', TfidfTransformer(use_idf=use_idf)), \n",
    "                ('model', self.model)])\n",
    "        self._cluster = self.cluster_pipeline.fit(self.data)\n",
    "        \n",
    "        self._train_time = time.time() - current_time\n",
    "        \n",
    "        \n",
    "        print(\"Training completed\")\n",
    "        print(\"Training time for {} : {} secs\".format(self.model_name, self._train_time))\n",
    "        \n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict cluster id for all clusters\n",
    "        \"\"\"\n",
    "        self.label_id_dict = {}\n",
    "        self.document_id_dict = {}\n",
    "        self.cluster_id = []\n",
    "        for i, document in enumerate(self.data):\n",
    "            cluster_id = self.cluster_pipeline.predict([document])[0]\n",
    "            self.label_id_dict.update({self.labels[i]:cluster_id})\n",
    "            self.document_id_dict.update({document:cluster_id})\n",
    "            self.cluster_id.append(cluster_id)\n",
    "        \n",
    "        return self.label_id_dict, self.document_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=cluster_number, init = \"k-means++\", max_iter=150, n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_cluster = DocumentCluster(data=documents,model=model,labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_cluster.train_model(ngram=2, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_label, result_docs = doc_cluster.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
