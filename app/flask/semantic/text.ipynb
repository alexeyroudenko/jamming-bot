{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jammingbot is a fantasy about a post-apocalyptic future, when the main functions of the Internet and assistant bots are defeated and only one self-replicating bot remains, aimlessly plowing the Internet. This is a bot that has no goal, only a path.\n",
      "Top-5 Keywords:\n",
      "[('internet', 2), ('bot', 2), ('fantasy', 1), ('post', 1), ('-', 1)]\n",
      "A bot -- short for robot and also called an internet bot -- is a computer program that operates as an agent for a user or other program or to simulate a human activity. Bots are normally used to automate certain tasks, meaning they can run without specific instructions from humans.\n",
      "Top-5 Keywords:\n",
      "[('bot', 2), ('program', 2), ('short', 1), ('robot', 1), ('internet', 1)]\n",
      "Business leaders say about 15% of their web application resources are taken up in dealing with the impact of automated bots on their organisations – but half of traffic on the world wide web is now thought to be generated by bots. So it is clear that many are vastly underestimating the impact on their organisations, according to new research by Netacea, a specialist in bot detection and mitigation.\n",
      "Top-5 Keywords:\n",
      "[('web', 2), ('impact', 2), ('bots', 2), ('organisations', 2), ('business', 1)]\n",
      "Duration: 01:33When available, closed caption (subtitles) language settings can be chosen using the Settings or CC icon on this video player.\n",
      "Top-5 Keywords:\n",
      "[('duration', 1), ('available', 1), ('closed', 1), ('caption', 1), ('subtitles', 1)]\n",
      "Agentless bot management making trusted, autonomous anti-bot security a reality for enterprises across websites, apps and APIs.\n",
      "Top-5 Keywords:\n",
      "[('bot', 2), ('agentless', 1), ('management', 1), ('autonomous', 1), ('anti', 1)]\n",
      "Self-healing networks: The next evolution in network managementThe age of AI is highlighting the security risks and pitfalls of traditional network management techniques. Self-healing networks may offer a solution that not only bolsters security, but makes IT and network teams more efficientRansomware payment value fell over 30% in 2024Several factors, including the impact of law enforcement operations disrupting cyber criminal gangs and better preparedness among users, may be behind a significant drop in the total value of ransomware paymentsNRF review: no escaping AI for retail in 2025What themes will be seen in the retail tech space over the next year, according to the 2025 National Retail Federation’s Big Show?Tech job postings dropped in 2024, according to researchJob postings for several IT roles returned to pre-pandemic levels last year, dropping when compared with 2023View All Stories\n",
      "Top-5 Keywords:\n",
      "[('network', 3), ('self', 2), ('networks', 2), ('security', 2), ('value', 2)]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "steps = glob.glob(\"../../../data/txt/*.html\")\n",
    "ii = 0\n",
    "\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "def get_text_from_html(html):    \n",
    "    soup = BeautifulSoup(html)\n",
    "    header = soup.find('header')\n",
    "    if header:\n",
    "        header.decompose()\n",
    "    footer = soup.find('footer')\n",
    "    if footer:\n",
    "        footer.decompose()\n",
    "    text_out = \"\"\n",
    "    for header in soup.find_all('h2'):\n",
    "        nextNode = header\n",
    "        while True:\n",
    "            nextNode = nextNode.nextSibling\n",
    "            if nextNode is None:\n",
    "                break\n",
    "            if isinstance(nextNode, NavigableString):\n",
    "                text_out += nextNode.strip().replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "                break\n",
    "            if isinstance(nextNode, Tag):\n",
    "                if nextNode.name in {'h1', 'h2', 'h3', 'h4', 'h5'}:\n",
    "                    break\n",
    "                txt = nextNode.get_text(strip=True).strip().replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "                if len(txt) > 0:\n",
    "                    text_out += nextNode.get_text(strip=True).strip().replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
    "                    break\n",
    "                    # return text_out\n",
    "                    \n",
    "    if text_out == \"\":\n",
    "        paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "        if len(paragraphs) > 0:\n",
    "            text_out = paragraphs[0]\n",
    "                            \n",
    "    return text_out\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "for step_html_path in steps:\n",
    "    html = open(step_html_path, 'r').read()    \n",
    "    txt = get_text_from_html(html)\n",
    "    print(txt)\n",
    "    if txt == \"\":\n",
    "        print(\"false\")\n",
    "    else:\n",
    "\n",
    "        # import spacy\n",
    "        # nlp = spacy.load(\"en_core_web_lg\")\n",
    "        # doc = nlp(txt)\n",
    "        # for token in doc:\n",
    "        #     print(token.text, \"→\", token.dep_, \"→\", token.head.text)\n",
    "        \n",
    "        from collections import Counter    \n",
    "        import spacy\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "        doc = nlp(txt)\n",
    "        \n",
    "        keywords = [token.text.lower() for token in doc if token.pos_ in [\"NOUN\", \"ADJ\"] and not token.is_stop]\n",
    "        keyword_counts = Counter(keywords)\n",
    "\n",
    "        print(\"Top-5 Keywords:\")\n",
    "        print(keyword_counts.most_common(5))\n",
    "\n",
    "        \n",
    "        # noun_hrases =  [chunk.text for chunk in doc.noun_chunks]\n",
    "        # noun_hrases_out = []\n",
    "        # for i in noun_hrases:\n",
    "        #     if len(i) > 5:\n",
    "        #         noun_hrases_out.append(i)                \n",
    "        # # print(noun_hrases_out)\n",
    "        # # print(\"-------------\")\n",
    "        # print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "        # print(\"-------------\")\n",
    "        # print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "        # print(\"-------------\")\n",
    "        # # Find named entities, phrases and concepts\n",
    "        # for entity in doc.ents:\n",
    "        #     print(entity.text, entity.label_)            \n",
    "    \n",
    "    ii += 1\n",
    "    if ii > 5:\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
