{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install -U spacy\n",
    "# python -m spacy download en_core_web_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy.cli\n",
    "# spacy.cli.download(\"en_core_web_md\")\n",
    "# en_core_web_md\n",
    "# en_core_web_lg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jamming bot is a fantasy about a post-apocalyptic future, when the core functions of the internet and assistant bots are defeated and only one self-replicating bot remains, aimlessly browsing the internet. It is a bot that has no goal, only a path. Currently, spiders, crawlers and bots serve a service purpose. They act as search engines, collect information, automate the infrastructure of the internet. Jamming bot is a fantasy about a post-apocalyptic future, when the core functions of the internet and assistant bots are defeated and only one self-replicating bot remains, aimlessly browsing the internet, perhaps studying the general mood of humanity in the scraps of meaning on the pages of the internet. It is a bot that has no goal, only a path. Jamming Bot is a fascinating and slightly melancholic concept, where the last remaining bot represents the legacy of the digital age. Jamming Bot may symbolize the loneliness and permanence of technology that has transcended control and self-improvement. It is like an observer, dwelling among the fragments of information, trying to catch the residual traces of humanity in the scraps of text, in the ruined pages, perhaps even interpreting them as traces of a long-forgotten society. Its movement across the Internet can be seen as a reflection of human aspirations, searches, and experiences, as it accidentally intersects with different fragments of meaning that were once assembled into a single network.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrases, Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jamming bot', 'a fantasy', 'a post-apocalyptic future', 'the core functions', 'the internet', 'assistant bots', 'only one self-replicating bot', 'the internet', 'no goal', 'only a path', 'spiders', 'crawlers', 'a service purpose', 'search engines', 'information', 'the infrastructure', 'the internet', 'Jamming bot', 'a fantasy', 'a post-apocalyptic future', 'the core functions', 'the internet', 'assistant bots', 'only one self-replicating bot', 'the internet', 'the general mood', 'humanity', 'the scraps', 'meaning', 'the pages', 'the internet', 'no goal', 'only a path', 'Jamming Bot', 'a fascinating and slightly melancholic concept', 'the last remaining bot', 'the legacy', 'the digital age', 'Jamming Bot', 'the loneliness', 'permanence', 'technology', 'control', 'self-improvement', 'an observer', 'the fragments', 'information', 'the residual traces', 'humanity', 'the scraps', 'the ruined pages', 'traces', 'a long-forgotten society', 'Its movement', 'the Internet', 'a reflection', 'human aspirations', 'searches', 'experiences', 'different fragments', 'meaning', 'a single network']\n",
      "-------------\n",
      "Noun phrases: ['Jamming bot', 'a fantasy', 'a post-apocalyptic future', 'the core functions', 'the internet', 'assistant bots', 'only one self-replicating bot', 'the internet', 'It', 'a bot', 'that', 'no goal', 'only a path', 'spiders', 'crawlers', 'bots', 'a service purpose', 'They', 'search engines', 'information', 'the infrastructure', 'the internet', 'Jamming bot', 'a fantasy', 'a post-apocalyptic future', 'the core functions', 'the internet', 'assistant bots', 'only one self-replicating bot', 'the internet', 'the general mood', 'humanity', 'the scraps', 'meaning', 'the pages', 'the internet', 'It', 'a bot', 'that', 'no goal', 'only a path', 'Jamming Bot', 'a fascinating and slightly melancholic concept', 'the last remaining bot', 'the legacy', 'the digital age', 'Jamming Bot', 'the loneliness', 'permanence', 'technology', 'that', 'control', 'self-improvement', 'It', 'an observer', 'the fragments', 'information', 'the residual traces', 'humanity', 'the scraps', 'text', 'the ruined pages', 'them', 'traces', 'a long-forgotten society', 'Its movement', 'the Internet', 'a reflection', 'human aspirations', 'searches', 'experiences', 'it', 'different fragments', 'meaning', 'that', 'a single network']\n",
      "-------------\n",
      "Verbs: ['defeat', 'replicate', 'remain', 'browse', 'have', 'serve', 'act', 'collect', 'automate', 'defeat', 'replicate', 'remain', 'browse', 'study', 'have', 'remain', 'represent', 'symbolize', 'transcend', 'dwell', 'try', 'catch', 'ruin', 'interpret', 'forget', 'see', 'intersect', 'assemble']\n",
      "-------------\n",
      "only one CARDINAL\n",
      "only one CARDINAL\n",
      "Jamming Bot PERSON\n",
      "Jamming Bot PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(text)\n",
    "noun_hrases =  [chunk.text for chunk in doc.noun_chunks]\n",
    "noun_hrases_out = []\n",
    "\n",
    "for i in noun_hrases:\n",
    "    if len(i) > 5:\n",
    "        noun_hrases_out.append(i)\n",
    "        \n",
    "print(noun_hrases_out)\n",
    "print(\"-------------\")\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"-------------\")\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "print(\"-------------\")\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(text)\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Jamming bot' не найдено в тексте.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Ищем токен \"Jamming bot\"\n",
    "query_token = None\n",
    "for token in doc:\n",
    "    if token.text.lower() == \"jammingbot\":\n",
    "        query_token = token\n",
    "\n",
    "if query_token:\n",
    "    print(f\"Найден токен '{query_token.text} {query_token.nbor(1).text}'\")\n",
    "\n",
    "    # Слова, связанные с \"Jamming bot\"\n",
    "    print(\"\\nСлова, связанные с 'Jamming bot', и типы зависимостей:\")\n",
    "    for child in query_token.children:\n",
    "        print(f\"{child.text} ({child.dep_})\")\n",
    "\n",
    "    # Родитель \"Jamming bot\"\n",
    "    print(f\"\\nРодитель 'Jammingbot': {query_token.head.text} ({query_token.head.dep_})\")\n",
    "\n",
    "    # Полное предложение\n",
    "    print(\"\\nПредложение:\")\n",
    "    print(query_token.sent)\n",
    "else:\n",
    "    print(\"'Jamming bot' не найдено в тексте.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bot', 'internet', 'jamming', 'bots', 'self']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp_lg(text)\n",
    "keywords = [token.text.lower() for token in doc if token.pos_ in [\"NOUN\", \"ADJ\"] and not token.is_stop]\n",
    "keyword_counts = Counter(keywords)\n",
    "words = []\n",
    "for item in keyword_counts.most_common(5):\n",
    "    words.append(item[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity w 'bot':\n",
      "bot: 1.00\n",
      "bots: 0.82\n",
      "crawlers: 0.45\n",
      "spiders: 0.39\n",
      "Internet: 0.34\n",
      "internet: 0.34\n",
      "automate: 0.31\n",
      "search: 0.28\n",
      "engines: 0.26\n",
      "collect: 0.25\n",
      "It: 0.24\n",
      "post: 0.23\n",
      "only: 0.21\n",
      "replicating: 0.20\n",
      "no: 0.20\n",
      "that: 0.20\n",
      "browsing: 0.20\n",
      "one: 0.20\n",
      "path: 0.20\n",
      "meaning: 0.20\n",
      "This: 0.20\n",
      "when: 0.19\n",
      "where: 0.19\n",
      "main: 0.19\n",
      "self: 0.18\n",
      "is: 0.18\n",
      "They: 0.18\n",
      "perhaps: 0.18\n",
      "purpose: 0.18\n",
      "pages: 0.17\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "doc = nlp(text)\n",
    "query = nlp(\"bot\")\n",
    "similarities = {}\n",
    "for token in doc:\n",
    "    if token.has_vector and query.has_vector:\n",
    "        similarity = query.similarity(token)\n",
    "        if similarity > 0:  # Фильтрация значений, близких к нулю\n",
    "            similarities[token.text] = similarity\n",
    "\n",
    "sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Similarity w '{query.text}':\")\n",
    "for word, similarity in sorted_similarities[0:30]:\n",
    "    print(f\"{word}: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
